\section{Introduction}
The Weibull distribution, with probability density function
\begin{align}
\label{eqn:pdf}
	p(y | \bm{\theta}) = \left(\frac{k}{\lambda^k}\right) y^{k-1} \exp\left(-\left(\frac{y}{\lambda}\right)^k\right) ,
\end{align}
where $\bm{\theta} = \{k,\lambda\}$ and $k>0$ is the shape parameter and $\lambda>0$ is the scale parameter, is a popular distribution in analysis of survival data. Given data ${\bf y} = (y_1, \ldots, y_n)^\prime$, a common approach to estimate Weibull parameters $\bm{\theta}$ is the method of maximum likelihood where the parameters are set to values that maximise the log-likelihood of the data
\begin{equation}
\label{eqn:complete:nll}
\ell(\bm{\theta}) = -n \log \left(\frac{\lambda^k}{k}\right) + (k-1) \left(\sum_{i=1}^n \log y_i\right) - \sum_{i=1}^n \left(\frac{y_i}{\lambda}\right)^k
\end{equation}
In this setting, the maximum likelihood (ML) estimates of $k,\lambda$ are 
\begin{equation}
\label{eqn:ml:k}
\hat{\lambda}^{k}({\bf y}) = \frac{1}{n} \sum_{i=1}^n y_i^{k}, 
\end{equation}
where $\hat{k}({\bf y})$ is defined implicitly 
\begin{equation}
\label{eqn:ml:lambda}
 \frac{n}{k} + \sum_{i=1}^n \log y_i - \frac{n \sum_i y_i^k \log y_i}{\sum_i y_i^k} = 0
\end{equation}
and must therefore be obtained by numerical optimisation. 

It is well known that the maximum likelihood estimate of the shape parameter $k$ is biased for small sample sizes. Ross~\cite{Ross94} derived a simple adjustment formula for the ML estimate of $k$
\begin{equation}
\label{eqn:mle:kross}
	\hat{k}_{\rm R}({\bf y}) = \left(\frac{n-2}{n-0.68}\right) \hat{k}_{\rm ML}({\bf y}) ,
\end{equation}
which aims to reduce the bias and later extended his approach to censored data~\cite{Ross96}. Hirose~\cite{Hirose99} proposed an alternative bias correction method for data with no censoring that was derived by fitting a non-linear function to simulation results. Teimouri and Nadarajah~\cite{TeimouriNadarajah13} develop improved maximum likelihood estimates for the Weibull distribution based on record statistics. In contrast, Yang and Xie~\cite{YangXie03} use the modified profile likelihood proposed by Cox and Reid~\cite{CoxReid87,CoxReid92} to derive an alternative maximum likelihood estimate of $k$ (MLC) from
\begin{equation}
\label{eqn:ml:yangxie}
 \frac{n-2}{k} + \sum_{i=1}^n \log y_i - \frac{n \sum_i y_i^k \log y_i}{\sum_i y_i^k} = 0.
\end{equation}
Using simulations, Yang and Xie showed that their estimate of $k$ is less biased than the ML estimate and more efficient than the estimate (\ref{eqn:mle:kross}) proposed by Ross. In a follow-up paper, Shen and Yang~\cite{ShenYang15} developed a profile maximum likelihood estimate of $k$ for complete and censored samples and showed that it outperforms MLC in simulations with complete data.  
\subsection{Type I Censored Data}
In survival analysis, one commonly does not observe complete data and instead has joint realisations of the random variables $(Y = y, \Delta = \delta)$ where $Y = \min (T, c)$ and 
\begin{eqnarray*}
	\Delta &=& {\rm I}(T \leq c) = 
	\begin{cases}
    1, & \text{if } T \leq c \; ({\rm observed\; survival})\\
    0, & \text{if } T > c \; ({\rm observed\; censoring})
	\end{cases}
\end{eqnarray*}
where the random variable $T$ denotes survival time and $c > 0$ is the fixed censoring time. The data comprises the survival time $T=t$ of an item if this is less than the corresponding censoring time $c$ (i.e., $T \leq c$); otherwise, we only know that the item survived beyond time $c$ (i.e., $T > c$).

The log-likelihood of data $D = \{(y_1, \delta_1), \ldots, (y_n, \delta_n)\}$ under type I censoring is
\begin{align}
\ell(\bm{\theta})
&= d \log \left(\frac{k}{\lambda^k}\right) -\frac{1}{\lambda^k} \sum_{i=1}^n y_i^k + \sum_{i=1}^n \log y_i^{\delta_i (k-1)}
\label{eqn:censored:nll}
\end{align}
where $d = \sum_{i=1}^n \delta_i$. The maximum likelihood (ML) estimates of $k,\lambda$ are 
\begin{equation}
\hat{\lambda}^{k}({\bf y}) = \frac{1}{d} \sum_{i=1}^n y_i^{k}, 
\end{equation}
where $\hat{k}({\bf y})$ is obtained from
\begin{equation}
\label{eqn:mle:censored:kscore}
 \frac{d}{k} + \sum_{i=1}^n \delta_i \log y_i - \frac{d \sum_i y_i^k \log y_i}{\sum_i y_i^k} = 0 \, .
\end{equation}

As with the case of complete data, the maximum likelihood estimate of $k$ with type I censored data has large bias in small samples and for large amounts of censoring. Based on the modified profile likelihood, Yang and Xie~\cite{YangXie03} propose an alternative estimate of $k$ 
\begin{equation}
\label{eqn:ml:yangxie:censored}
 \frac{d-1}{k} + \sum_{i=1}^n \delta_i \log y_i - \frac{d \sum_i y_i^k \log y_i}{\sum_i y_i^k} = 0.
\end{equation}
We note that the above score function requires $d>1$ to yield a positive estimate for $k$. Yang and Xie demonstrate that the proposed estimate of $k$ is less biased and more efficient than the regular maximum likelihood estimate. 

Shen and Yang~\cite{ShenYang15} derived a new second- and third-order bias correction formula for the shape parameter of the Weibull distribution without censoring and with general right-censoring models. Although the new estimate is shown to be effective in correcting bias, it must be computed through bootstrap simulation. The same procedure was later extended to include Weibull regression with complete and general right censoring~\cite{ShenYang17}.

More recently, Choi et al~\cite{ChoiEtAl20} examine a different problem of Weibull parameter overestimation caused by mass occurrences of (censored) events in the early time period and develop an expectation maximization (EM) algorithm to reduce bias.
\section{A simple adjustment to maximum likelihood estimates to reduce estimation bias}
In a landmark paper, Cox and Snell~\cite{CoxSnell68} derived an approximation to the finite sample bias of ML estimates for independent, but not necessarily identically distributed, data. Let $\bm{\theta} \in \mathbb{R}^p$, where $p > 0$ is the number of free parameters, which is $p=2$ in the case of the Weibull model. Cox and Snell showed that the bias for the $s$-th element of the ML estimate $\hat{\theta}_{\rm ML}$ can be written as
\begin{equation}
    \left[ \text{Bias}(\hat{\theta}_{\rm ML}) \right]_{s} = \sum_{i=1}^p \sum_{j=1}^p \sum_{l=1}^p \kappa^{s,i} \kappa^{j,l} \left( \frac{1}{2} \kappa_{ijl} + \kappa_{ij,l}\right) + O(n^{-2})
\end{equation}
for $s = 1,\ldots,p$, where the cumulants are
\begin{align}
    \kappa_{ij} &= \mathbb{E}\left\{ \frac{\partial^2 \ell(\bm{\theta})}{\partial \theta_i \partial \theta_j} \right\}, \quad 
    %
    \kappa_{ijl} = \mathbb{E}\left\{ \frac{\partial^3 \ell(\bm{\theta})}{\partial \theta_i \partial \theta_j \partial \theta_l} \right\}, \\
    %
    \kappa_{ij,l} &= \mathbb{E}\left\{ \frac{\partial^2 \ell(\bm{\theta})}{\partial \theta_i \partial \theta_j} \frac{\partial \ell(\bm{\theta})}{\partial \theta_l}\right\},
\end{align}
for $i,j = 1,\ldots, p$ and $\kappa^{i,j}$ is the $(i,j)$-th entry of the \emph{inverse} of the expected Fisher information matrix ${\bf K} = \{ -\kappa_{ij} \}$. Following Cordeiro and Klein~\cite{CordeiroKlein94}, we can compactly write this in matrix notation as
\begin{equation}
\label{eqn:CoxSnell}
    \text{Bias}(\hat{\theta}_{\rm ML}) = {\bf K}^{-1} {\bf A} \text{vec}({\bf K}^{-1}) + O(n^{-2}),
\end{equation}
where the matrix ${\bf A}$ is the $(p \times p^2)$ matrix given by
\begin{align}
    \label{eq:Cordeiro:Klein:A}
    {\bf A} &= \left[ {\bf A}^{(1)} | {\bf A}^{(2)} | \cdots | {\bf A}^{(p)} \right], \quad 
    {\bf A}^{(l)} = \{a_{ij}^{(l)} \} \\
    \quad a_{ij}^{(l)} &= \kappa_{ij}^{(l)} - \frac{1}{2} \kappa_{ijl}, \quad \kappa_{ij}^{(l)} = \frac{\partial \kappa_{ij}}{ \partial \theta_l} 
\end{align}
for $i,j,l = 1,\ldots, p$. The ML estimate with reduced bias $\bm{\tilde{\theta}}_{\rm ML}$ is then
\begin{align}
    \nonumber
    \bm{\tilde{\theta}}_{\rm ML} &= \hat{\bm{\theta}}_{\rm ML} - \hat{\bf K}^{-1} \hat{\bf A} \text{vec}(\hat{\bf K}^{-1}), \\
    \label{eqn:ml:bias:adjustment}
    &= \hat{\bm{\theta}}_{\rm ML} - \text{Bias}(\hat{\theta}_{\rm ML}) ,
\end{align}
where $\hat{\bf K}$ and $\hat{\bf A}$ are evaluated at the usual maximum likelihood estimate $\hat{\bm{\theta}}_{\rm ML}$. A benefit of this bias approximation formula is that it can be computed even if the maximum likelihood estimate is not available in closed form. A similar approach to the above was used to derive bias-adjusted maximum likelihood estimates for the unit Weibull distribution~\cite{MenezesEtAl21} and the inverse Weibull distribution~\cite{MazucheliEtAl18} with complete data only.
\begin{thm}
\label{thm:jointpdf}
The finite sample bias of the maximum likelihood estimate (\ref{eqn:ml:k}) for the Weibull distribution with complete data is 
\begin{align}
\text{\rm Bias}(\hat{k}_{\rm ML}) &= k \left(\frac{18 \left(\pi ^2-2 \zeta (3)\right)}{n \pi ^4}\right) + O(n^{-2})\\
&\approx k \left(\frac{1.3795}{n}\right) \label{eqn:ml:k:adjusted}
\end{align}
where $\zeta(\cdot)$ is the Riemann zeta function and $\gamma \approx 0.5772$ is the Euler--Mascheroni constant. Maximum likelihood estimates of $k$ and $\lambda$ with reduced bias can be obtained from (\ref{eqn:ml:bias:adjustment}). 
\begin{proof}
The proof involves the application of the Cordeiro and Klein~\cite{CordeiroKlein94} approach, given by (\ref{eqn:CoxSnell}) and (\ref{eq:Cordeiro:Klein:A}), to the Weibull distribution (\ref{eqn:pdf}). It is well known that expected Fisher information matrix for the Weibull distribution is
\begin{align*}
    {\bf K} &= \left(
\begin{array}{cc}
 \frac{\left(6 (\gamma -1)^2+\pi ^2\right) n}{6 k^2} & \frac{(\gamma -1) n}{\lambda } \\
 \frac{(\gamma -1) n}{\lambda } & \frac{k^2 n}{\lambda ^2} \\
\end{array}
\right), \\
{\bf K}^{-1} &= \left(
\begin{array}{cc}
 \frac{6 k^2}{\pi ^2 n} & -\frac{6 (\gamma -1) \lambda }{\pi ^2 n} \\
 -\frac{6 (\gamma -1) \lambda }{\pi ^2 n} & \frac{\left(6 (\gamma -1)^2+\pi ^2\right) \lambda ^2}{\pi ^2 k^2 n} \\
\end{array}
\right) .
\end{align*}
By direct calculation, we can show that the matrix ${\bf A}$ is
\begin{equation*}
\left(
\begin{array}{cccc}
 \frac{n \left(-12 \zeta (3)-3 \gamma  \left(2 \gamma  (\gamma -7)+\pi ^2+16\right)+7 \pi ^2+12\right)}{12 k^3} & -\frac{\left(6 \gamma  (\gamma -4)+\pi ^2+12\right) n}{12 k \lambda } & -\frac{\left(6 \gamma  (\gamma -4)+\pi ^2+12\right) n}{12 k \lambda } & \frac{(-\gamma  k+3 k+\gamma -1) n}{2 \lambda ^2} \\
 -\frac{\left(6 \gamma  (\gamma -4)+\pi ^2+12\right) n}{12 k \lambda } & -\frac{(\gamma  k+k+\gamma -1) n}{2 \lambda ^2} & \frac{(-\gamma  k+3 k+\gamma -1) n}{2 \lambda ^2} & -\frac{(k-1) k^2 n}{2 \lambda ^3} \\
\end{array}
\right)
\end{equation*}
Substituting ${\bf K}^{-1}$ and ${\bf A}$ into (\ref{eqn:CoxSnell}) and simplifying completes the proof.
\end{proof}
\end{thm}
We observe that the maximum likelihood estimate of $k$ is biased upward for any finite $n$. An advantage of the proposed bias adjusted estimate is that it can readily be computed in any software that implements ML Weibull estimation.

\begin{thm}
\label{thm:typeI}
The finite sample bias of the maximum likelihood estimate (\ref{eqn:ml:k}) for the Weibull distribution with type I censored data is 
\begin{align}
\text{\rm Bias}(\hat{k}_{\rm ML}) &= k\left(\frac{f(p)}{n}\right) + O(n^{-2})
\end{align}
where $f(p)$ is a somewhat lengthy and complicated function of the proportion of uncensored observations $p = 1-\exp(-\left(c/\lambda )\right)^k)$. A simple approximation to $f(p)$ based on rational functions is 
\begin{equation*}
    f(p) \approx \frac{-580.684 p^3+4690.74 p^2-20743.7 p+18830}{-17026.8 p^2+18804.5 p+1}
\end{equation*}
with the absolute approximation error being less than $0.003$ for all $0.05 \leq p \leq 0.95$. As with complete data, maximum likelihood estimate of $k$ with reduced bias can be obtained from (\ref{eqn:ml:bias:adjustment}).
\end{thm}
\begin{figure}[t]
\begin{center}
\includegraphics[width=6.0cm]{figs/bias_function.pdf}
\end{center}
\caption{Bias adjustment $f(p)$ for the maximum likelihood estimate of the Weibull distribution shape parameter $k$ of as a function of the proportion of uncensored observations $p=1-\exp(-(c/\lambda)^k)$. \label{fig:biasf}}
\end{figure}
The proof is straightforward involving the application of the Cordeiro and Klein~\cite{CordeiroKlein94} bias adjustment formulation to the Weibull distribution with type I censoring and is therefore omitted. Figure~\ref{fig:biasf} shows the bias adjustment $f(p)$ as a function of the proportion of uncensored observations $p$. As the proportion of uncensored observations $p \to 1$ (ie, no censoring), $f(p) \to (\approx) 1.3795$ as expected. Additionally, $f(p) \to \infty$ as the proportion of censored data is increased (ie, $p \to 0$).
\subsection{Simulation}
\label{eqn:sim}
\begin{table*}[ht]
\scriptsize
\begin{center}
\begin{tabular}{cccccccccccccccc} 
\toprule
$n$ & $k^*$ & \multicolumn{4}{c}{Bias} & & \multicolumn{4}{c}{Mean Squared Error} & & \multicolumn{4}{c}{KL divergence}\\
  &   & ML & MLC & MLP & MMLE & ~ & ML & MLC & MLP & MMLE  & ~ & ML & MLC & MLP & MMLE\\
\cmidrule{1-16}
\multirow{4}{*}{10}  &  0.5 &  0.085 &  0.008 & {\bf  0.001} &  0.004 &  &  0.038 &  0.023 & {\bf  0.022} &  0.023 &  &  0.268 &  0.169 & {\bf  0.164} &  0.165\\ 
 &  1.0 &  0.170 &  0.017 & {\bf  0.002} &  0.009 &  &  0.151 &  0.092 & {\bf  0.089} &  0.090 &  &  0.272 &  0.169 &  0.164 & {\bf  0.158}\\ 
 &  5.0 &  0.852 &  0.088 & {\bf  0.014} &  0.045 &  &  3.775 &  2.305 & {\bf  2.241} &  2.268 &  &  0.264 &  0.167 &  0.162 & {\bf  0.152}\\ 
 & 10.0 &  1.701 &  0.172 & {\bf  0.026} &  0.087 &  & 15.102 &  9.218 & {\bf  8.978} &  9.079 &  &  0.266 &  0.168 &  0.164 & {\bf  0.153}\\ 
\vspace{-2mm} \\ 
\cmidrule{2-16}
\vspace{-2mm} \\ 
\multirow{4}{*}{20}  &  0.5 &  0.038 &  0.004 & {\bf  0.001} &  0.001 &  &  0.012 &  0.009 & {\bf  0.009} &  0.009 &  &  0.072 &  0.061 & {\bf  0.060} &  0.061\\ 
 &  1.0 &  0.077 &  0.009 & {\bf  0.001} &  0.003 &  &  0.048 &  0.037 & {\bf  0.037} &  0.037 &  &  0.072 &  0.061 &  0.060 & {\bf  0.059}\\ 
 &  5.0 &  0.382 &  0.042 & {\bf  0.004} &  0.011 &  &  1.203 &  0.928 & {\bf  0.916} &  0.917 &  &  0.072 &  0.061 &  0.060 & {\bf  0.058}\\ 
 & 10.0 &  0.755 &  0.075 & {\bf  0.000} &  0.014 &  &  4.769 &  3.686 & {\bf  3.636} &  3.639 &  &  0.072 &  0.061 &  0.060 & {\bf  0.058}\\ 
\vspace{-2mm} \\ 
\cmidrule{2-16}
\vspace{-2mm} \\ 
\multirow{4}{*}{40}  &  0.5 &  0.014 &  0.002 & {\bf  0.000} &  0.000 &  &  0.004 &  0.003 &  0.003 & {\bf  0.003} &  &  0.023 &  0.022 & {\bf  0.022} &  0.022\\ 
 &  1.0 &  0.029 &  0.003 & {\bf -0.000} &  0.000 &  &  0.015 &  0.013 &  0.013 & {\bf  0.013} &  &  0.023 &  0.021 &  0.021 & {\bf  0.021}\\ 
 &  5.0 &  0.143 &  0.016 & {\bf  0.001} &  0.001 &  &  0.367 &  0.329 &  0.328 & {\bf  0.327} &  &  0.023 &  0.022 &  0.021 & {\bf  0.021}\\ 
 & 10.0 &  0.290 &  0.036 & {\bf  0.005} &  0.006 &  &  1.458 &  1.308 &  1.300 & {\bf  1.299} &  &  0.023 &  0.022 &  0.022 & {\bf  0.021}\\ 
\vspace{-3mm} \\ 
\bottomrule
\vspace{+1mm}
\end{tabular}
\caption{Bias, mean squared error and Kullback--Leibler (KL) divergence for maximum likelihood (ML),  conditional maximum likelihood of Yang and Xie (MLC),  profile maximum likelihood of Shen and Yang (MLP) and our bias-adjusted maximum likelihood (MMLE) estimates of $k^*$ computed over $10^5$ simulations runs with $\lambda^* = 1$.\label{tab:results:complete}}
\end{center}
\end{table*}
We performed a simulation to examine the finite sample behaviour of the new bias-adjusted ML estimates of $k$ for both complete and type I censored data. 

\subsubsection{Complete data}
For each run of the simulation, we generated $n$ data points from the model Weibull$(k^*, \lambda^* = 1)$ where $n = \{10, 20, 50\}$ and the shape parameter was set to $k^* \in \{0.5, 1, 5, 10\}$. Maximum likelihood estimates, our proposed bias-adjusted maximum likelihood estimates (MMLE), conditional maximum likelihood estimates (MLC) proposed by Yang and Xie~\cite{YangXie03}, and the profile maximum likelihood estimates of Shen and Yang (MLP)~\cite{ShenYang15} were then computed from the data. We used the second-order bias reduction of Shen and Yang as it was virtually indistinguishable from the third-order formula in our tests. We performed $10^5$ simulations for each combination of $(k^*, n)$ and recorded the average bias, mean squared error and Kullback--Leibler (KL) divergence~\cite{KullbackLeibler51} from the data generating model. In the case of the Weibull distribution, the KL divergence between the data generating model Weibull$(k_0, \lambda_0)$ and approximating model Weibull$(k_1, \lambda_1)$ is 
\begin{align*}
{\rm KL}( k_0, \lambda_0 ||  k_1, \lambda_1) &= \left(\frac{\lambda_0 }{\lambda _1}\right)^{k_1} \left( \frac{k_1}{k_0} \right) \Gamma \left( \frac{k_1}{k_0} \right) + \left(\frac{k_1}{k_0}-1\right) \gamma \\
&+ \log \left(\frac{k_0}{k_1} \left(\frac{\lambda _1}{\lambda_0 }\right)^{k_1}\right)-1   .
\end{align*}
All simulation results are shown in Table~\ref{tab:results:complete}.

All three bias-adjusted maximum likelihood estimates of $k$ result in a significant reduction in bias compared to the usual ML estimate. Compared to MLC, our  estimate yields improved mean squared error and KL divergence, especially as $k$ increases. The profile maximum likelihood estimate has a slightly smaller bias than our estimate, while the mean squared error and the KL divergence for the two estimates are virtually identical. Unlike both the MLC and MLP estimates, our bias-adjusted maximum likelihood estimate of $k$ is simple to compute in software via existing Weibull ML estimation procedures and does not require the use of the parametric bootstrap.
\subsubsection{Type I censored data}
For each run of the simulation, we generated $n$ data points from the model Weibull$(k^*, \lambda^* = 1)$ where $n = \{10, 20, 50\}$ and the shape parameter was set to $k^* \in \{0.5, 1, 5, 10\}$. The proportion of uncensored observations was set to $p \in \{0.3, 0.5, 0.7, 0.9\}$. In addition to the bias and the mean squared error in estimating the shape parameter, we computed the Kullback--Leibler (KL) divergence~\cite{KullbackLeibler51} between the data generating model and each estimated model:
the KL divergence between two Weibull models Weibull($k_0, \lambda_0$) and Weibull($k_1, \lambda_1$) is
\begin{align*}
\label{eqn:weibull:kl}
{\rm KL}( k_0, \lambda_0 ||  k_1, \lambda_1) 
&= \exp(-\left(c/\lambda_0\right)^{k_0}) A_1 + \left(\frac{\lambda_0 }{\lambda _1}\right)^{k_1} A_2 \\
&+ \left(1-\frac{k_1}{k_0}\right) A_3 +\log \left(\frac{k_0}{k_1} \left(\frac{\lambda _1}{\lambda_0 }\right)^{k_1}\right)-1 ,
\end{align*}
where
\begin{align*}
A_1 &= \log \left(\frac{k_1 }{k_0}c^{k_1-k_0} \lambda_0^{k_0} \lambda_1^{-k_1}\right)+\left(\frac{c}{\lambda _1}\right){}^{k_1}+1  ,\\
A_2 &= \Gamma \left(\frac{k_1}{k_0}+1\right)-\Gamma \left(\frac{k_1}{k_0}+1,\left(\frac{c}{\lambda }\right)^k\right)  , \\
A_3 &= \text{Ei}\left(-\left(\frac{c}{\lambda_0 }\right)^{k_0}\right)-\gamma ,
\end{align*}
and $\text{Ei}(\cdot)$ is the exponential integral function
\begin{equation}
	\text{Ei}(z) = -\int_{-z}^\infty \frac{\exp(-t)}{t} \, dt .
\end{equation}
The newly proposed bias adjustment estimate of $k$ (MMLE) was again compared to the standard ML estimate, the conditional maximum likelihood estimate (MLC) proposed by Yang and Xie~\cite{YangXie03} and the profile maximum likelihood estimate (MLP) of Shen and Yang~\cite{ShenYang15}. The third-order profile maximum likelihood estimate had issues with numerical stability for small $n$ and large amounts of censoring sometimes resulting in a negative estimate of $k^*$, hence all the comparisons were made with the second-order variant. We restricted the experiments to exclude data sets where the number of uncensored observations $d (=\sum_i \delta_i) < 2$, as MLC may result in negative estimates of $k$ for $d < 2$. The results of these simulations, averaged over $10^5$ runs for each combination of $(n,p,k^*)$, are shown in Table~\ref{tab:results:censored}. 

We observe that our MMLE estimate of $k$ is more efficient and less biased than the standard ML of $k$ for all tested values of $(n,p,k^*)$. The conditional maximum likelihood estimate of $k$ is, in general, more biased and has higher mean squared error compared to the MLP and our MMLE estimates. In terms of bias reduction, the profile maximum likelihood estimate of $k$ is virtually identical to our MMLE for $n \geq 30$. For small sample sizes ($n = 20$) and higher levels of censoring ($p \leq 0.5$), the MMLE estimate appears superior to MLC and MLP in terms of bias, mean squared error and KL divergence. Additionally, our MMLE estimate is easily computed without the need for numerical simulation in any software that implements fitting of the Weibull distribution to complete and type I censored data. 
\begin{table*}[tbph]
\scriptsize
\begin{center}
\begin{tabular}{ccccccccccccccccccccc} 
\toprule
$n$ & $p$ & $k^*$ & \multicolumn{4}{c}{Bias} & & \multicolumn{4}{c}{Mean Squared Error} & & \multicolumn{4}{c}{KL Divergence} \\
    &     &     & ML & MLC & MLP & MMLE & ~ & ML & MLC & MLP & MMLE & ~ & ML & MLC & MLP & MMLE \\
\multirow{16}{*}{20} & \multirow{4}{*}{ 0.3} &  0.5 &  0.115 &  0.021 &  0.004 & {\bf  0.002} &  &  0.220 &  0.150 &  0.090 & {\bf  0.090} &  &  0.070 &  0.060 &  0.062 & {\bf  0.053}\\ 
 & &  1.0 &  0.228 &  0.040 &  0.005 & {\bf -0.001} &  &  0.605 &  0.401 &  0.303 & {\bf  0.301} &  &  0.070 &  0.060 &  0.061 & {\bf  0.053}\\ 
 & &  5.0 &  1.156 &  0.214 &  0.033 & {\bf  0.008} &  & 14.757 &  9.683 &  7.292 & {\bf  7.248} &  &  0.070 &  0.061 &  0.061 & {\bf  0.053}\\ 
 & & 10.0 &  2.251 &  0.374 &  0.058 & {\bf -0.007} &  & 55.591 & 36.196 & 32.112 & {\bf 28.799} &  &  0.070 &  0.062 &  0.061 & {\bf  0.054}\\ 
 & \multirow{4}{*}{ 0.5} &  0.5 &  0.051 & {\bf  0.001} &  0.001 & -0.003 &  &  0.037 &  0.028 &  0.028 & {\bf  0.028} &  &  0.059 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & &  1.0 &  0.108 &  0.007 &  0.008 & {\bf  0.000} &  &  0.144 &  0.109 &  0.110 & {\bf  0.108} &  &  0.059 &  0.055 &  0.052 & {\bf  0.051}\\ 
 & &  5.0 &  0.556 &  0.051 &  0.053 & {\bf  0.014} &  &  3.672 &  2.785 &  2.785 & {\bf  2.738} &  &  0.060 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & & 10.0 &  1.095 &  0.084 &  0.085 & {\bf  0.009} &  & 15.172 & 11.561 & 11.571 & {\bf 11.377} &  &  0.060 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & \multirow{4}{*}{ 0.7} &  0.5 &  0.034 & {\bf -0.002} &  0.003 & -0.003 &  &  0.019 & {\bf  0.015} &  0.016 &  0.016 &  &  0.056 &  0.054 & {\bf  0.051} &  0.051\\ 
 & &  1.0 &  0.075 &  0.003 &  0.013 & {\bf  0.002} &  &  0.081 & {\bf  0.065} &  0.068 &  0.066 &  &  0.058 &  0.055 & {\bf  0.052} &  0.052\\ 
 & &  5.0 &  0.381 &  0.023 &  0.075 & {\bf  0.017} &  &  2.048 & {\bf  1.660} &  1.730 &  1.676 &  &  0.058 &  0.055 & {\bf  0.052} &  0.052\\ 
 & & 10.0 &  0.681 & {\bf -0.028} &  0.073 & -0.042 &  &  7.474 & {\bf  6.122} &  6.363 &  6.181 &  &  0.056 &  0.053 & {\bf  0.050} &  0.050\\ 
 & \multirow{4}{*}{ 0.9} &  0.5 &  0.032 & {\bf  0.001} &  0.012 &  0.001 &  &  0.013 &  0.011 &  0.012 & {\bf  0.011} &  &  0.062 &  0.058 &  0.054 & {\bf  0.054}\\ 
 & &  1.0 &  0.063 & {\bf  0.001} &  0.023 &  0.002 &  &  0.051 &  0.042 &  0.045 & {\bf  0.042} &  &  0.062 &  0.058 &  0.054 & {\bf  0.054}\\ 
 & &  5.0 &  0.314 & {\bf  0.001} &  0.111 &  0.008 &  &  1.320 & {\bf  1.082} &  1.165 &  1.083 &  &  0.061 &  0.057 &  0.054 & {\bf  0.054}\\ 
 & & 10.0 &  0.632 & {\bf  0.006} &  0.226 &  0.019 &  &  5.306 & {\bf  4.346} &  4.680 &  4.349 &  &  0.062 &  0.058 &  0.054 & {\bf  0.054}\\ 
\vspace{-2mm} \\ 
\cmidrule{2-17}
\vspace{-2mm} \\ 
\multirow{16}{*}{20} & \multirow{4}{*}{ 0.3} &  0.5 &  0.114 &  0.020 &  0.003 & {\bf -0.000} &  &  0.262 &  0.177 &  0.185 & {\bf  0.102} &  &  0.070 &  0.061 &  0.061 & {\bf  0.053}\\ 
 & &  1.0 &  0.231 &  0.042 &  0.006 & {\bf  0.001} &  &  0.756 &  0.504 &  0.339 & {\bf  0.337} &  &  0.071 &  0.061 &  0.062 & {\bf  0.053}\\ 
 & &  5.0 &  1.149 &  0.207 &  0.036 & {\bf  0.002} &  & 16.835 & 11.123 &  9.821 & {\bf  7.900} &  &  0.070 &  0.061 &  0.061 & {\bf  0.053}\\ 
 & & 10.0 &  2.312 &  0.427 &  0.108 & {\bf  0.014} &  & 82.069 & 54.570 & 59.320 & {\bf 37.053} &  &  0.070 &  0.062 &  0.061 & {\bf  0.053}\\ 
 & \multirow{4}{*}{ 0.5} &  0.5 &  0.054 &  0.003 &  0.003 & {\bf -0.000} &  &  0.037 &  0.028 &  0.028 & {\bf  0.028} &  &  0.059 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & &  1.0 &  0.109 &  0.008 &  0.008 & {\bf  0.000} &  &  0.151 &  0.115 &  0.114 & {\bf  0.112} &  &  0.060 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & &  5.0 &  0.534 &  0.030 &  0.029 & {\bf -0.009} &  &  3.720 &  2.843 &  2.826 & {\bf  2.780} &  &  0.060 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & & 10.0 &  1.091 &  0.081 &  0.080 & {\bf  0.004} &  & 15.178 & 11.578 & 11.507 & {\bf 11.321} &  &  0.060 &  0.056 &  0.053 & {\bf  0.051}\\ 
 & \multirow{4}{*}{ 0.7} &  0.5 &  0.036 &  0.001 &  0.006 & {\bf  0.000} &  &  0.020 & {\bf  0.016} &  0.017 &  0.016 &  &  0.057 &  0.055 & {\bf  0.051} &  0.052\\ 
 & &  1.0 &  0.072 &  0.001 &  0.011 & {\bf -0.000} &  &  0.078 & {\bf  0.064} &  0.067 &  0.065 &  &  0.057 &  0.055 & {\bf  0.051} &  0.052\\ 
 & &  5.0 &  0.366 &  0.009 &  0.061 & {\bf  0.003} &  &  1.990 & {\bf  1.619} &  1.692 &  1.640 &  &  0.058 &  0.055 & {\bf  0.052} &  0.052\\ 
 & & 10.0 &  0.724 &  0.012 &  0.114 & {\bf -0.002} &  &  7.781 & {\bf  6.334} &  6.608 &  6.408 &  &  0.057 &  0.054 & {\bf  0.051} &  0.051\\ 
 & \multirow{4}{*}{ 0.9} &  0.5 &  0.031 & {\bf  0.000} &  0.011 &  0.001 &  &  0.013 &  0.011 &  0.012 & {\bf  0.011} &  &  0.062 &  0.058 &  0.054 & {\bf  0.054}\\ 
 & &  1.0 &  0.063 & {\bf  0.001} &  0.023 &  0.002 &  &  0.053 & {\bf  0.043} &  0.047 &  0.043 &  &  0.062 &  0.058 &  0.054 & {\bf  0.054}\\ 
 & &  5.0 &  0.309 & -0.004 &  0.106 & {\bf  0.003} &  &  1.298 & {\bf  1.066} &  1.146 &  1.066 &  &  0.061 &  0.057 &  0.054 & {\bf  0.054}\\ 
 & & 10.0 &  0.632 & {\bf  0.006} &  0.225 &  0.019 &  &  5.283 &  4.332 &  4.662 & {\bf  4.332} &  &  0.061 &  0.058 &  0.054 & {\bf  0.054}\\ 
\vspace{-2mm} \\ 
\cmidrule{2-17}
\vspace{-2mm} \\ 
\multirow{16}{*}{30} & \multirow{4}{*}{ 0.3} &  0.5 &  0.065 &  0.007 & {\bf  0.001} & -0.001 &  &  0.056 &  0.041 &  0.037 & {\bf  0.037} &  &  0.042 &  0.038 &  0.038 & {\bf  0.034}\\ 
 & &  1.0 &  0.133 &  0.016 &  0.004 & {\bf -0.000} &  &  0.336 &  0.257 &  0.262 & {\bf  0.179} &  &  0.042 &  0.039 &  0.038 & {\bf  0.034}\\ 
 & &  5.0 &  0.653 &  0.068 &  0.009 & {\bf -0.009} &  &  5.621 &  4.184 &  3.907 & {\bf  3.788} &  &  0.041 &  0.038 &  0.038 & {\bf  0.034}\\ 
 & & 10.0 &  1.334 &  0.161 &  0.045 & {\bf  0.008} &  & 23.086 & 17.180 & 16.069 & {\bf 15.252} &  &  0.042 &  0.038 &  0.038 & {\bf  0.034}\\ 
 & \multirow{4}{*}{ 0.5} &  0.5 &  0.034 &  0.001 &  0.002 & {\bf -0.000} &  &  0.020 & {\bf  0.017} &  0.017 &  0.017 &  &  0.037 &  0.036 &  0.034 & {\bf  0.034}\\ 
 & &  1.0 &  0.070 &  0.004 &  0.006 & {\bf  0.001} &  &  0.081 &  0.067 &  0.068 & {\bf  0.067} &  &  0.037 &  0.036 &  0.034 & {\bf  0.034}\\ 
 & &  5.0 &  0.341 &  0.015 &  0.025 & {\bf  0.000} &  &  2.019 &  1.680 &  1.698 & {\bf  1.679} &  &  0.037 &  0.036 &  0.034 & {\bf  0.034}\\ 
 & & 10.0 &  0.684 &  0.032 &  0.051 & {\bf  0.002} &  &  8.095 & {\bf  6.732} &  6.815 &  6.740 &  &  0.037 &  0.036 &  0.034 & {\bf  0.034}\\ 
 & \multirow{4}{*}{ 0.7} &  0.5 &  0.024 &  0.001 &  0.004 & {\bf  0.000} &  &  0.012 & {\bf  0.010} &  0.010 &  0.010 &  &  0.036 &  0.035 & {\bf  0.034} &  0.034\\ 
 & &  1.0 &  0.047 &  0.001 &  0.007 & {\bf  0.000} &  &  0.046 & {\bf  0.040} &  0.041 &  0.041 &  &  0.036 &  0.035 & {\bf  0.034} &  0.034\\ 
 & &  5.0 &  0.235 &  0.004 &  0.038 & {\bf  0.001} &  &  1.170 & {\bf  1.020} &  1.050 &  1.030 &  &  0.036 &  0.035 & {\bf  0.034} &  0.034\\ 
 & & 10.0 &  0.475 &  0.012 &  0.080 & {\bf  0.006} &  &  4.662 & {\bf  4.058} &  4.179 &  4.096 &  &  0.036 &  0.035 & {\bf  0.034} &  0.034\\ 
 & \multirow{4}{*}{ 0.9} &  0.5 &  0.020 & -0.001 &  0.006 & {\bf -0.000} &  &  0.008 &  0.007 &  0.007 & {\bf  0.007} &  &  0.038 &  0.036 &  0.035 & {\bf  0.035}\\ 
 & &  1.0 &  0.040 & {\bf -0.000} &  0.014 &  0.000 &  &  0.031 &  0.027 &  0.029 & {\bf  0.027} &  &  0.038 &  0.036 &  0.035 & {\bf  0.035}\\ 
 & &  5.0 &  0.198 & -0.003 &  0.065 & {\bf -0.001} &  &  0.778 &  0.684 &  0.716 & {\bf  0.684} &  &  0.038 &  0.036 &  0.035 & {\bf  0.035}\\ 
 & & 10.0 &  0.402 & {\bf -0.000} &  0.137 &  0.005 &  &  3.133 &  2.750 &  2.881 & {\bf  2.748} &  &  0.038 &  0.037 &  0.035 & {\bf  0.035}\\ 
\vspace{-2mm} \\ 
\cmidrule{2-17}
\vspace{-2mm} \\ 
\multirow{16}{*}{40} & \multirow{4}{*}{ 0.3} &  0.5 &  0.047 &  0.004 &  0.002 & {\bf  0.001} &  &  0.033 &  0.026 &  0.025 & {\bf  0.025} &  &  0.029 &  0.028 &  0.027 & {\bf  0.025}\\ 
 & &  1.0 &  0.094 &  0.009 &  0.004 & {\bf  0.001} &  &  0.130 &  0.103 &  0.100 & {\bf  0.099} &  &  0.029 &  0.028 &  0.027 & {\bf  0.025}\\ 
 & &  5.0 &  0.465 &  0.039 &  0.014 & {\bf  0.001} &  &  3.329 &  2.653 &  2.541 & {\bf  2.528} &  &  0.029 &  0.028 &  0.028 & {\bf  0.025}\\ 
 & & 10.0 &  0.937 &  0.084 &  0.035 & {\bf  0.009} &  & 13.143 & 10.449 & 10.102 & {\bf 10.053} &  &  0.029 &  0.028 &  0.027 & {\bf  0.025}\\ 
 & \multirow{4}{*}{ 0.5} &  0.5 &  0.024 & {\bf  0.000} &  0.001 & -0.001 &  &  0.014 & {\bf  0.012} &  0.012 &  0.012 &  &  0.027 &  0.026 &  0.026 & {\bf  0.025}\\ 
 & &  1.0 &  0.049 &  0.001 &  0.003 & {\bf -0.001} &  &  0.055 & {\bf  0.048} &  0.048 &  0.048 &  &  0.027 &  0.026 &  0.026 & {\bf  0.025}\\ 
 & &  5.0 &  0.248 &  0.007 &  0.017 & {\bf -0.001} &  &  1.382 & {\bf  1.204} &  1.217 &  1.208 &  &  0.027 &  0.026 &  0.026 & {\bf  0.025}\\ 
 & & 10.0 &  0.507 &  0.025 &  0.045 & {\bf  0.009} &  &  5.550 & {\bf  4.826} &  4.882 &  4.842 &  &  0.027 &  0.026 &  0.026 & {\bf  0.025}\\ 
 & \multirow{4}{*}{ 0.7} &  0.5 &  0.018 &  0.001 &  0.003 & {\bf  0.000} &  &  0.008 & {\bf  0.007} &  0.008 &  0.008 &  &  0.027 &  0.026 & {\bf  0.025} &  0.025\\ 
 & &  1.0 &  0.034 & {\bf -0.000} &  0.005 & -0.001 &  &  0.033 & {\bf  0.030} &  0.030 &  0.030 &  &  0.027 &  0.026 & {\bf  0.025} &  0.025\\ 
 & &  5.0 &  0.180 &  0.008 &  0.033 & {\bf  0.006} &  &  0.830 & {\bf  0.747} &  0.764 &  0.752 &  &  0.027 &  0.026 & {\bf  0.025} &  0.025\\ 
 & & 10.0 &  0.354 &  0.011 &  0.062 & {\bf  0.007} &  &  3.306 & {\bf  2.977} &  3.043 &  2.998 &  &  0.027 &  0.026 & {\bf  0.025} &  0.025\\ 
 & \multirow{4}{*}{ 0.9} &  0.5 &  0.015 & {\bf  0.000} &  0.005 &  0.000 &  &  0.006 &  0.005 &  0.005 & {\bf  0.005} &  &  0.027 &  0.027 &  0.026 & {\bf  0.026}\\ 
 & &  1.0 &  0.030 & {\bf  0.000} &  0.010 &  0.000 &  &  0.022 &  0.020 &  0.021 & {\bf  0.020} &  &  0.027 &  0.026 &  0.026 & {\bf  0.026}\\ 
 & &  5.0 &  0.150 & {\bf  0.002} &  0.052 &  0.003 &  &  0.558 &  0.505 &  0.523 & {\bf  0.505} &  &  0.028 &  0.027 &  0.026 & {\bf  0.026}\\ 
 & & 10.0 &  0.296 & {\bf -0.001} &  0.099 &  0.002 &  &  2.222 &  2.017 &  2.085 & {\bf  2.015} &  &  0.027 &  0.027 &  0.026 & {\bf  0.026}\\ 

\vspace{-3mm} \\ 
\bottomrule
\vspace{+1mm}
\end{tabular}
\caption{Bias, mean squared error and Kullback--Leibler (KL) divergence for maximum likelihood (ML),  conditional maximum likelihood of Yang and Xie (MLC),  profile maximum likelihood of Shen and Yang (MLP) and our bias-adjusted maximum likelihood (MMLE) estimates of $k^*$ computed over $10^5$ simulations runs with $\lambda^* = 1$; $p$ denotes the proportion of uncensored observations. \label{tab:results:censored}}
\end{center}
\end{table*}
















\bibliographystyle{unsrtnat}
