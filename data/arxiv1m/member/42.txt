\section{Introduction}

We study $r$-nets, a powerful tool in computational and metric geometry, with several applications in approximation algorithms.
An $r$-net for a metric space $(X,\norm{\cdot}), \, |X|=n$ and for numerical parameter $r$ is a subset $R\subseteq X$ 
such that the closed $r/2$-balls centered at the points of $R$ are disjoint, and the closed $r$-balls around the same points cover all of $X$. We define approximate $r$-nets analogously. Formally,

\begin{dfn}
Given a pointset $X \subseteq {\mathbb R}^d$, a distance parameter $r \in {\mathbb R}$ and an approximation parameter $\epsilon >0$, a $(1+\epsilon)r$-net of $X$ is a subset $R \subseteq X$ s.t. the following properties hold:
\begin{enumerate}
\item (packing) For every $p, q \in R$, $p \neq q $, we have that $\norm{p-q}_2 \geq r$.
\item (covering) For every $p \in X$, there exists a $q \in R$ s.t. $\norm{p-q}_2 \leq (1+\epsilon)r$.
\end{enumerate}
\end{dfn}

\paragraph{Previous Work.} 
Finding $r$-nets can be addressed naively by considering all points of $X$ unmarked and, while there remains an unmarked point $p$, the algorithm adds it to $R$ and marks all other points within distance $r$ from  
$p$. The performance of this algorithm can be improved by using grids and hashing \cite{HP04}. 
However, their complexity remains too large when dealing with big data in high dimension. The naive algorithm is quadratic in $n$ and the grid approach is in $O(d^{d/2} n)$, hence it is relevant only for constant dimension $d$~\cite{HR15}. 
In \cite{HPM05}, they show that an approximate net hierarchy for an 
arbitrary finite metric can be computed in $O(2^{ddim}n \log n)$, where $ddim$ is the doubling dimension.
This is satisfactory when doubling dimension is constant, but requires a vast amount of resources when it is high.

When the dimension is high, there is need for algorithms with time complexity polynomial in $d$ and subquadratic in $n$. One approach, which computes $(1+\epsilon)r$-nets in high dimension is that of \cite{EHS15}, which uses the Locality Sensitive Hashing (LSH) method of
\cite{AI08}. The resulting time complexity is
$\tilde{O}(dn^{2-\Theta(\epsilon)})$, where $\epsilon>0$ is quite small and $\tilde{O}$ hides polylogarithmic factors. 

In general, high dimensional analogues of  classical geometric problems 
have been mainly addressed by LSH. For instance, the approximate closest pair problem can be trivially solved by performing $n$ approximate nearest neighbor (ANN) queries. For sufficiently small $\epsilon$, this costs $\tilde{O}(dn^{2-\Theta(\epsilon)})$ time, due to the complexity factor of an LSH query. Several other problems have been reduced to ANN queries \cite{GIV01}. Recently, Valiant \cite{Val12}, \cite{Val15} presented an algorithm for the approximate closest pair problem 
in time $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$. This is a different approach in the sense that while LSH exploits dimension reduction through random projections, the algorithm of \cite{Val15} is inspired by high dimensional phenomena. One main step of the algorithm is that of projecting the pointset up to a higher dimension. 


\paragraph{Our Contribution.} 
We present a new randomized algorithm that computes approximate $r$-nets in time subquadratic in $n$ and polynomial in the dimension, and improves upon the complexity of the best known algorithm.
Our method does not employ LSH and, with probability $1-o(1)$, it returns 
$R\subset X$, which is a $(1+\epsilon)r$-net of $X$. 

We reduce the problem of an approximate $r$-net for arbitrary vectors (points) under Euclidean distance to the same problem for vectors on the unit sphere. Then, depending on the magnitude of distance $r$, an algorithm handling ``small" distances or an algorithm handling ``large" distances is called. 
These algorithms reduce the Euclidean problem of $r$-nets on unit vectors to finding an $r$-net for unit vectors under inner product (Section~\ref{SGeneral}). This step requires that the multiplicative $1+\epsilon$ approximation of the distance corresponds to an additive $c\epsilon$ approximation of the inner product, for suitable constant $c>0$.

Next, we convert the vectors having unit norm into vectors with entries $\{-1, +1\}$ (Section \ref{SInner}).
This transformation is necessary in order to apply the Chebyshev embedding of \cite{Val15}, an embedding that damps the magnitude of the inner product of ``far" vectors, while preserving the magnitude of the inner product of ``close"
vectors. 
For the final step of the algorithm, we first 
apply a procedure that allows us to efficiently 
compute $(1+\epsilon)$-nets in the case where the number of ``small" distances is large. Then, we apply a modified version of the {\tt Vector Aggregation} algorithm of \cite{Val15}, that exploits fast 
matrix multiplication, so as to achieve the desired running time.

In short, we extend Valiant's framework \cite{Val15} and we compute $r$-nets in time $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$, thus improving on the exponent of the LSH-based construction \cite{EHS15}, when $\epsilon$ is small enough. This improvement by $\sqrt{\epsilon}$ in the exponent is the same as the complexity improvement obtained in \cite{Val15} over the LSH-based algorithm for the approximate closest pair problem. 

Our study is motivated by the observation that computing efficiently an $r$-net leads to efficient solutions for several geometric problems, specifically in approximation algorithms. In particular, our extension of $r$-nets in high dimensional Euclidean space can be plugged in the framework of~\cite{HR15}. The new framework has many applications, notably the $k$th nearest neighbor distance problem, which we solve in $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$.


\paragraph{Paper Organization.}
Section \ref{SInner} presents an algorithm for 
computing an approximate net with respect to the inner product for a set of unit vectors.
Section~\ref{SGeneral} translates the problem of finding $r$-nets under Euclidean distance to the same problem under inner product. In Section \ref{Sapps}, we discuss applications of our construction and possible future work. Omitted proofs are included in the Appendices.

We use $\norm{\cdot}$ to denote the Euclidean norm $\norm{\cdot}_2$ throughout the paper. 

\section{Points on a sphere under inner product}\label{SInner}
In this section, we design an algorithm for constructing an approximate $\rho$-net of vectors on the sphere under inner product.
To that end, we reduce the problem to constructing an approximate net under absolute inner product for vectors that lie on the vertices of a unit hypercube.

Since our ultimate goal is a solution to computing $r$-nets with respect to Euclidean distance, we allow additive error in the approximation, which under certain assumptions, translates to multiplicative error in Euclidean distance. 
In the following, we define rigorously the notion of approximate $\rho$-nets under inner product.

\begin{dfn}
\label{DfnNetInn}
For any $X\subset {\mathbb S}^{d-1}$, an approximate $\rho$-net for $(X,\langle \cdot,\cdot\rangle)$ , 
 with additive approximation parameter $\epsilon>0$, is a subset $C\subseteq X$ which satisfies the following properties:
 \begin{itemize}
 \item for any two $p \neq q \in C$, $\langle p, q \rangle < \rho$, and
  \item for any $x\in X$, there exists $p \in C$ s.t. $\langle x, p \rangle \geq \rho-\epsilon$.
 \end{itemize}

\end{dfn}
One relevant notion is that of $\epsilon$-kernels \cite{AHV05}. In $\epsilon$-kernels, one is interested in finding a subset of the input pointset, which approximates its directional width. Such constructions have been extensively studied when the dimension is low, due to their relatively small size.  

\subsection{Crude approximate nets}

In this subsection we develop our basic tool, which is based on the Vector Aggregation Algorithm by \cite{Val15}. This tool aims to compute  approximate $\rho$-nets with multiplicative error, as opposed to what we have set as our final goal for this section, namely to bound additive error. 
Moreover, in the context of this subsection, two vectors are close to each other when the magnitude 
of their inner product is large, and two vectors are 
far from each other when the magnitude of their inner product is small.
Let $|\langle \cdot,\cdot \rangle|$ denote the magnitude of the inner product of two vectors.

\begin{dfn} \label{DfnMagnInnNet}
 For any $X=[x_1,\ldots, x_n], X' =[x_1',\ldots,x_n'] \subset {\mathbb R}^{d \times n}$, a crude approximate $\rho$-net for $(X,X',|\langle \cdot,\cdot \rangle|)$, 
 with multiplicative approximation factor $c>1$, is a subset $C\subseteq [n]$ which satisfies the following properties:
 \begin{itemize}
    \item for any two $i \neq j \in C$, $|\langle x_i, x_j' \rangle| < c \rho$, and
\item for any $i\in [n]$, there exists $j \in C$ s.t.\ $|\langle x_i, x_j' \rangle| \geq \rho$.
 \end{itemize}

\end{dfn}

{\tt Vector Aggregation} follows the 
exposition of \cite{Val15}. 
The main difference is that, instead of the ``compressed'' matrix $Z^T Z$, we use the form $X^T Z$, where $Z$ derives from vector aggregation.
Both forms encode the information in the Gram matrix $X^T X$.
The matrix $X^TZ$ is better suited for our purposes, since each row corresponds to an input vector instead of an aggregated subset; this extra information may be useful in further problems.

\begin{framed}

{\tt Vector Aggregation}\\

Input: $X =[x_1,\ldots,x_n] \in {\mathbb R}^{d \times n}$, 
$X' =[x_1',\ldots,x_n'] \in {\mathbb R}^{d \times n}$, 
$\alpha \in (0,1)$, $\tau>0$. 

Output: $n\times n^{1-\alpha}$ matrix $W$ and random partition $S_1 , \ldots , S_{n^{1-\alpha}}$ of $\{x_1,\ldots,x_n\}$.



  \begin{itemize}
   \item Randomly partition $[n]$ into $n^{1-\alpha}$ disjoint subsets, each of size $n^{\alpha}$ , denoting the
sets $S_1 , \ldots , S_{n^{1-\alpha}}$.
   \item For each $i = 1, 2, \ldots , 78 \log n$:
   \begin{itemize}
    \item Select $n$ coefficients $q_1 ,\ldots , q_n \in \{-1, +1\}$ at random.
    \item Form the $d\times n^{1-\alpha}$ matrix $Z^i$ with entries $z_{j,k}^i=\sum_{l\in S_k} q_l \cdot x_{j,l}'$ 
    \item $W^i=X^T Z^i$
   \end{itemize}

   \item Define the $n \times n^{1-\alpha}$ matrix $W$ with $w_{i,j} =quartile(|w_{i,j}^1|,\ldots|w_{i,j}^{78 \log n}|)$. 
   \item Output $W$ and $S_1 , \ldots , S_{n^{1-\alpha}}$.
  \end{itemize}
 
\end{framed}


\begin{thm}\label{ThmVecAgg}
Let $X \in {\mathbb R}^{d \times n}$, 
$X'  \in {\mathbb R}^{d \times n}$, 
$\alpha \in (0,1)$, $\tau>0$ the input of {\tt Vector Aggregation}. Then, the algorithm returns a matrix $W$ of size $n\times n^{1-\alpha}$ and a random partition $S_1 , \ldots , S_{n^{1-\alpha}}$, which with probability $1-O(1/n^3)$ satisfies the following:
\begin{itemize}
\item For all $j\in [n]$ and $k\in [n^{1-\alpha}]$, if $\forall u \in S_k$, $ |\langle x_j, u \rangle|\leq \tau$ then $|w_{j,k}| < 3 \cdot n^{\alpha} \tau$.
\item For all $j\in [n]$ and $k\in [n^{1-\alpha}]$ 
if $\exists u\in S_k$, $|\langle x_j, u \rangle|\geq 3n^{\alpha}\tau$ then $|w_{j,k}| \geq 3 \cdot n^{\alpha} \tau$.
\end{itemize}
Moreover, the algorithm runs in time 
$\tilde{O}(dn+n^{2-\alpha}+MatrixMul( n \times d,d \times n^{1-\alpha}))$.
\end{thm}



For the case of pointsets with many ``small" distances, 
we rely crucially on the fact that the expected 
number of near neighbors for a randomly chosen point 
is large. 
So, if we iteratively choose random points and 
 delete these and their neighbors, we will end up with 
 a pointset which satisfies the property of having sufficiently few ``small" distances. Then, we apply {\tt Vector Aggregation}.


\begin{framed}
{\tt Crude ApprxNet}\\

Input: $X =[x_1,\ldots,x_n] \in {\mathbb R}^{d \times n}$, 
$X' =[x_1',\ldots,x_n'] \in {\mathbb R}^{d \times n}$, 
$\alpha \in (0,1)$, $\tau>0$. 

Output: $C'\subseteq [n]$, {$F' \subseteq [n]$}.

\begin{itemize}
\item $C\gets \emptyset$, $F_1 \gets \emptyset, F_2 \gets \{x_1,\ldots,x_n\}$
\item Repeat $n^{0.5}$ times:
 \begin{itemize}
 \item Choose a column $x_i$ uniformly at random.
 \item $C \gets C \cup \{x_i\}$.
  \item Delete column $i$ from matrix $X$ and column $i$ from matrix $X'$.
 \item Delete each column $k$ from matrix $X$, $X'$ s.t. $|\langle x_i, x_k'
 \rangle| \geq \tau$.
  \item If there is no column $k$ from matrix $X$ s.t. $|\langle x_i, x_k'\rangle| \geq \tau$, then $F_1 \gets F_1 \cup \{x_i\}$
  \end{itemize}


\item Run {\tt Vector Aggregation} 
with input $X$, $X'$, $\alpha$, $\tau$ and output 
$W$, $S_1,\ldots,S_{n^{1-\alpha}}$.
\item For each of the remaining columns $i=1,\ldots$:
   \begin{itemize}
    \item For any $|w_{i,j}|\geq 3 n^{\alpha} \tau$:
     \begin{itemize}
     \item If more than $n^{1.7}$ times in here, output "ERROR".
      \item Compute inner products between $x_i$ and vectors in $S_j$. 
      For each vector $x_k' \in S_j$ s.t. $x_k' \neq x_i$ and $|\langle x_i,x_k'\rangle|\geq \tau$, 
      delete row $k$ {and $F_2 \gets F_2 \backslash \{ x_i\}$.}
     \end{itemize}
     \item $C \gets C \cup \{x_i\} $ 
   \end{itemize}
\item Output indices of $C$ {and $F \gets \{F_1 \cup F_2 \}$}.
\end{itemize}

\end{framed}

\begin{thm}\label{ThmCrudeNet}
  On input $X =[x_1,\ldots,x_n] \in {\mathbb R}^{d \times n}$, 
$X' =[x_1',\ldots,x_n'] \in {\mathbb R}^{d \times n}$, 
$\alpha \in (0,1)$, $\tau>0$, {\tt Crude ApprxNet}, computes a crude  $3n^{\alpha}$-approximate $\tau$-net for $X$, $X'$, following the notation of Definition \ref{DfnMagnInnNet}. 
The algorithm costs time:
$$
\tilde{O}(n^{2-\alpha}+ d \cdot n^{1.7+\alpha}+MatrixMul( n \times d,d \times n^{1-\alpha})),
$$
and succeeds with probability $1-O(1/n^{0.2})$. 
Additionally, it outputs a set $F\subseteq R$ with the following property:
$\{x_i \mid \forall x_j \neq x_i~ |\langle x_j,x_i \rangle | <  \tau \}\subseteq F \subseteq \{x_i \mid \forall x_j \neq x_i~ |\langle x_j,x_i \rangle | < n^a\tau \}$.
\end{thm}


\begin{proof}

We perform $n^{0.5}$ iterations and for each, we compare the inner products between the randomly chosen vector and all other vectors. Hence, the time needed is $O(dn^{1.5})$.

In the following, we denote by 
$X_i$ the number of vectors which have ``large" magnitude of the inner product with the randomly chosen point in the $i$th iteration. 
Towards proving correctness, 
suppose first that ${\mathbb E}[X_i]>2n^{0.5}$ for all $i=1, \ldots n^{0.5}$. The expected number of vectors we delete in each 
iteration of the algorithm is more than $2n^{0.5}+1$. So, after $n^{0.5}$ iterations,
the expected total number of deleted vectors will be greater than $n$. This means
that if the hypothesis holds for all iterations we 
will end up with a proper net. 

Now suppose that there is an iteration $j$ where ${\mathbb E}[X_j] \leq 2n^{0.5}$. After all iterations, the 
number of ``small" distances are at most $n^{1.5}$ on expectation. By Markov's inequality, 
when the 
{\tt Vector Aggregation} algorithm is called, the following is satisfied
with probability 
$1-n^{-0.2}$ : 
$$
|\{(i,k) \mid |\langle x_i, x_k'\rangle| \geq\tau, i\neq k\}| \leq n^{1.7} .
$$
By Theorem \ref{ThmVecAgg}
and the above discussion, the number of entries in the matrix $W$ that we need to visit is at most $n^{1.7}$. For each entry, we perform a brute force 
which costs $d n^\alpha$. 

Now notice that the first iteration stores centers $c$ and deletes all points $p$ for which $|\langle c,p\rangle| \geq \tau$. Hence,  any two centers $c,c'$ satisfy $ |\langle c,p\rangle| < \tau$. In the second iteration, over the columns of $W$, notice that by Theorem \ref{ThmVecAgg}, for any two centers $c,c'$ we have $|\langle c,c'\rangle| <3 n^{\alpha}\tau.$
\end{proof}




\subsection{Approximate inner product nets}

In this subsection, we show that the problem of computing $\rho$-nets for the inner product of unit 
vectors reduces to the less natural problem of Definition \ref{DfnMagnInnNet}, which refers to the magnitude of the inner product.

The first step consists of mapping the unit vectors to vectors in $\{-1,1\}^{d'}$. The mapping is essentially Charikar's LSH scheme \cite{Cha02}. 
Then, we apply the Chebyshev embedding of~\cite{Val15} in order to achieve gap amplification, and finally 
we call algorithm {\tt Crude ApprxNet}, which will now return 
a proper $\rho$-net with additive error.  
\begin{thm}[\cite{Val15}] \label{ThmUnif}
There exists an algorithm with the following properties. Let $d'=O(\frac{\log n}{\delta^2})$
and $Y \in {\mathbb R}^{d'\times n}$ denote its output 
on input $X$, $\delta$, where $X$ is a
matrix whose columns have unit norm, with probability $1 - o(1/n^2 )$, for all pairs $i, j \in [n]$,
$
\Big|{\langle Y_i , Y_j \rangle}/{d'}-\Big(1-2 \cdot {\mathrm{cos}^{-1}(\langle X_i,X_j \rangle)}/{ \pi}\Big)\Big| \leq \delta
,$
where $X_i$, $Y_i$ denote the $i$th column of $X$ and $Y$ respectively. Additionally, the runtime of 
the algorithm is $O( \frac{d n \log n}{\delta^2})$.
\end{thm}

The following theorem provides a randomized embedding that damps the magnitude of the inner product of ``far" vectors, while preserving the magnitude of the inner product of ``close"
vectors. The statement is almost verbatim that of~\cite[Prop.6]{Val15} except that we additionally establish an asymptotically better probability of success. The proof is the same, but since we claim stronger guarantees on success probability, we include the complete proof in Appendix~\ref{AppThmCheb}.

\begin{thm}\label{ThmCheb}
Let $Y$, $Y'$ be the matrices output by algorithm ``Chebyshev Embedding" on
input $X, X' \in \{-1,1\}^{d\times n} , \tau^{+}\in [-1,1] , \tau^{-} \in [-1,1]$ with $\tau^{-}<\tau^{+}$ , integers $q, d'$. 
With probability
{$ 1 - o(1/n)$} over the randomness in the construction of
$Y, Y'$, for all $i, j \in [n]$, 
$\langle Y_i , Y_j' \rangle$ is within $\sqrt{d'} \log n$ from the value 
$
T_q \Big(\frac{\langle X_i, X_j'\rangle/d' - \tau^{-}}{\tau^{+}-\tau^{-}}2 -1  \Big) \cdot d' \cdot (\tau^{+}-\tau^{-})^q  /{2^{3q-1}},
$
where $T_q$ is the degree-$q$ Chebyshev polynomial of the first kind. The algorithm runs in
time $O(d' \cdot n\cdot q)$.
\end{thm}

\begin{framed}
{\tt Inner product ApprxNet}\\

Input: $X =[x_1,\ldots,x_n] $ with each $x_i \in {\mathbb S}^{d-1}$, $\rho \in [-1,1]$, $\epsilon \in (0,1/2]$.

Output: Sets $C, F \subseteq [n]$. 



\begin{itemize}
\item If $\rho\leq \epsilon$, then:
\begin{itemize}
\item $C \gets \emptyset$, $F\gets \emptyset$, $W \gets \{x_1,\ldots,x_n\}$
\item While $W\neq \emptyset$:
\begin{itemize}
\item Choose arbitrary vector $x\in W$.
\item $W \gets W \setminus \{y \in W \mid \langle x,y\rangle \geq \rho-\epsilon \}$
\item $C \gets C \cup \{x\}$
\item If $\forall y \in W$, $\langle x,y \rangle<\rho-\epsilon $ then $F\gets F. \cup \{x\}$
\end{itemize}
\item Return indices of $C$, $F$.
\end{itemize}
\item Apply Theorem \ref{ThmUnif} for input $X$, $\delta=\epsilon/2 \pi$  
and output $Y\in \{-1,1\}^{d' \times n}$ for $d'=O(\log n/\delta^2)$.
\item Apply Theorem \ref{ThmCheb} 
for input $Y$, $d''= n^{0.2}$, $q=50^{-1} \log n$, 
$\tau^-=-1$, $\tau^{+}=1-\frac{2 \cos^{-1}(\rho-\epsilon)}{\pi} +\delta$
and output $Z, Z'$. 
\item Run algorithm {\tt Crude ApprxNet} 
with input $\tau=3n^{0.16}$, $\alpha=\sqrt{\epsilon}/500$, $Z,Z'$ and output $C$, $F$.
\item Return $C$, $F$.
\end{itemize}
\end{framed}

\begin{thm}\label{AppIPnet}
The algorithm {\tt Inner product ApprxNet}, on input  $X =[x_1,\ldots,x_n] $ with each $x_i \in {\mathbb S}^{d-1}$, $\rho \in [-1,1]$ and $\epsilon \in (0,1/2]$,
computes an approximate $\rho$-net with additive 
error 
$\epsilon$, using the notation of Definition \ref{DfnNetInn}. The algorithm runs in time $\tilde{O}(dn+n^{2-\sqrt{\epsilon}/600})$ and 
succeeds with probability $1-O(1/n^{0.2})$. Additionally, it computes a set $F$ with the following property:
$\{x_i \mid \forall x_j \neq x_i~ \langle x_j,x_i \rangle  < \rho -\epsilon \}\subseteq F \subseteq \{x_i \mid \forall x_j \neq x_i~ \langle x_j,x_i \rangle < \rho  \}$.
\end{thm}



\section{Approximate nets in high dimensions}
\label{SGeneral}
In this section, we translate the problem of computing $r$-nets in $({\mathbb R}^d,\|\cdot \|)$ to the problem of computing 
$\rho$-nets for unit vectors under inner product. One intermediate step is that of computing $r$-nets for 
unit vectors under Euclidean distance. 

\subsection{From arbitrary to unit vectors}
In this subsection, we show that if one is interested in 
finding an $r$-net for $({\mathbb R}^d,\|\cdot \|)$, it is sufficient 
to solve the problem for points on the unit sphere. One analogous statement is used in \cite{Val15}, where they prove that one can apply a randomized mapping from the general Euclidean space to points on a unit sphere, while preserving the ratio of distances for any two pairs of points. The claim derives by the simple observation that an $r$-net in the initial space can be approximated 
by computing an $\epsilon r/c$-net on the sphere, where $c$ 
is the maximum norm of any given point envisaged as vector. Our exposition is even simpler since we can directly employ the analogous theorem from \cite{Val15}.

\begin{cor}\label{Standardize}
There exists an algorithm, {\tt Standardize}, which, on input a $d \times n$ matrix $X$ with entries $x_{i,j} \in {\mathbb R}$, a constant $\epsilon \in (0, 1)$ and a distance parameter $r \in {\mathbb R}$, outputs a $m'\times n $ matrix $Y$, with columns having unit norm and $m'=\log^3n$, and a distance parameter $\rho \in {\mathbb R} $, such that a $\rho$-net of $Y$ is an approximate $r$-net of $X$, with probability $1-o(1/poly(n))$.
\end{cor}

\begin{comment}
\begin{framed}
\textbf{Standardize}\\

Input: A $d \times n$ matrix $X$ with entries $x_{i,j} \in {\mathbb R}$, a constant $\epsilon \in (0, 1)$, a distance parameter $r \in {\mathbb R}$.

Output: A $m'\times n $ matrix $Y$ with columns having unit norm and $m'=\log^3n$, and a distance parameter $\rho \in {\mathbb R} $, such that our algorithm computes an $r$-net of $X$ given a $\rho$-net of $Y$.

\begin{itemize}
\item Define two $d$-dimensional vectors $X_{n+1}, X_{n+2}$, s.t.\ $r'=X_{n+1}-X_{n+2}$ and $\|r'\|=r$, and let matrix $X'$ denote the concatenation of $X$, $X_{n+1}$ and $X_{n+2}$ with size $d\times n+2$.

\item Perform a Johnson-Lindenstrauss transformation on the columns of $X'$, projecting them to dimension $m'$, so as to yield matrix $X''$.

\item Let $c$ denote the magnitude of the largest column of $X''$. Choose a random $m'$-dimensional vector $u$ of magnitude $8c/\epsilon$.

\item Let matrix $Y$ be the result of adding $u$ to each column of $X''$ and normalizing all columns so as to have unit norm.
\item Define $\rho:=\|Y_{n+1}-Y_{n+2}\|$ to be the new distance parameter.
\end{itemize}
\end{framed}
\end{comment}

\subsection{Approximate nets under Euclidean distance}
In this subsection, we show that one can translate the problem of computing an $r$-net for points on the unit sphere under Euclidean distance, to finding an $r$-net for unit vectors under  inner product as defined in Section \ref{SInner}. Moreover, we  identify the subset of the $r$-net which contains 
the centers that are approximately far from any other point. Formally,

\begin{dfn}
Given a set of points $X$ and $\epsilon>0$, 
a set $F\subseteq X$ of $(1+\epsilon)$-approximate $r$-far points is defined by the following property:  $\{x\in X \mid \forall x \neq y \in X ~ \|x-y\| > (1+\epsilon)r \}\subseteq F \subseteq \{x\in X \mid \forall x \neq y \in X ~ \|x-y\| > r \}$.
\end{dfn}

If $r$ is greater than some constant, the problem can be immediately solved by the law of cosines. If $r$ cannot be considered as constant, we distinguish cases $r\geq 1/n^{0.9}$ and $r <1/n^{0.9}$. The first case 
is solved by a simple modification of an analogous algorithm in \cite[p.13:28]{Val15}. The second case is not straightforward and requires partitioning the pointset in a manner which allows computing $r$-nets for each part separately. Each part has bounded diameter which implies that we need to solve a ``large $r$" subproblem.

\begin{thm}\label{ThmLargeRadius}
There exists an algorithm, {\tt ApprxNet(Large radius)}, which, 
for any constant $\epsilon\in (0,1/2]$, $X\subset {\mathbb S}^{d-1}$ s.t. $|X|=n$, outputs
a $(1+\epsilon)r$-net and a set of $(1+\epsilon)$-approximate $r$-far points with probability $1-O(1/n^{0.2})$. Additionally, 
provided $r>1/n^{0.9}$ the runtime of the algorithm is $\tilde{O}(dn^{2-\Theta(\sqrt[]{\epsilon})})$. 
\end{thm}

Let us now present an algorithm which translates the problem of finding an $r$-net for $r<1/n^{0.9}$ to the problem of computing an $r$-net for $r\geq 1/n^{0.9}$. The main idea is that we compute disjoint subsets $S_i$, which are far enough from each other, so that we can compute $r$-nets for each $S_i$ independently. We show that for each $S_i$ we can compute 
$T_i \subseteq S_i$ which has bounded diameter and 
$T_i'\subseteq S_i$ such that $T_i$, $T_i'$ are disjoint, each point in $T_i$ is far from each point in $T_i'$, and $|T_i'|\leq 3|S_i|/4$. It is then easy to find $r$-nets for $T_i$ by employing the ApprxNet(Large radius) algorithm. Then, we recurse on $T_i'$ which contains a constant fraction of points from $|S_i|$. Then, we cover points in 
$S_i \setminus(T_i \cup T_i')$ and points which do not belong to any $S_i$. 


\begin{framed}
{\tt ApprxNet(Small radius)}\\

  Input: $X =[x_1,\ldots,x_n]^T $ with each $x_i \in {\mathbb S}^{d-1}$, $r< 1/n^{0.9}$, $\epsilon \in (0,1/2]$.

Output: Sets $R, F \subseteq [n]$.

\begin{enumerate}
\item Project points on a uniform random unit vector 
and consider projections $p_1,\ldots,p_n$ which 
wlog correspond to $x_1,\ldots,x_n\in {\mathbb R}^d$.
\item
Traverse the list as follows
\begin{itemize}
\item If $|\{j  \mid p_j \in [p_i-r,p_i] \}| \leq n^{0.6}$ or $i=n$:
\begin{itemize}
\item If $|\{j  \mid  p_j <p_i  \}| \leq n^{0.9}$ remove 
from the list all points $p_j$ s.t. $p_j<p_i-r$ and save set $K=\{x_j \mid p_j\in [p_i-r,p_i] \}$.
\item If $|\{j  \mid p_j <p_i\}| > n^{0.9}$ save sets $K_i=\{x_j \mid p_j\in [p_i-r,p_i] \} \cup K$,
$S_i=\{x_j\mid p_j<p_i-r\}\setminus K$ and remove projections of $S_i$ and $K_i$ from the list.
\end{itemize}
\end{itemize}
\item After traversing the list if we have not saved any $S_i$ go to 5; otherwise for each $S_i$:
\begin{itemize}
\item For each $u\in S_i$, sample 
$n^{0.1}$ distances between $u$ and randomly chosen 
$x_k\in S_i$. Stop if for the selected $u\in S_i$, more than $1/3$ of the sampled points are in distance $\leq  r n^{0.6}$. This means that 
one has found $u$ s.t. 
$|\{x_k \in S_i, \|u-x_k\|\leq  r n^{0.6}\}| \geq |S_i|/4$ with high probability. If no such point was found, output "ERROR".
\item Let $0\leq d_1\leq \ldots\leq d_{|S_i|}$ be the 
distances between $u$ and all other points in $S_i$. 
Find $c\in[ r n^{0.6},2r  n^{0.6}]$ s.t. 
$|\{j \in [n] \mid d_j \in [c,c+r] \}| <n^{0.4}$,
 store $W_i=\{x_j \mid d_j \in [c,c+r] \}$, 
 and remove $W_i$ from $S_i$. 
\item Construct the sets $T_i=\{x_j \in S_i \mid d_j<c\}$ 
and $T_i'=\{x_j \in S_i \mid d_j > c+r\}$. 
\begin{itemize}
\item For $T_i$, subtract $u$ from all vectors in $T_i$, run {\tt Standardize}, then {\tt ApprxNet (Large radius)}, both with {$\epsilon/4$}. Save points which correspond to output at $R_i$, $F_i$ respectively.
\item Recurse on $T_i'$ the whole algorithm, and notice that $|T_i'|\leq 3 |S_i|/4$. Save output at $R_i'$, and $F_i'$ respectively.
\end{itemize}
\end{itemize}
\item 
Let $R \gets \bigcup_i R_i \cup R_i'$ and 
$F \gets \bigcup_i F_i \cup F_i'$. Return to the list $p_1,\ldots,p_n$.
\begin{enumerate}
\item {Remove from $F$ all points which cover at least one point from $\bigcup_i W_i$ or $\bigcup_i K_i$.}
\item \label{itm:deleteb}
Delete all points $ (\bigcup_i T_i) \setminus (\bigcup_i R_i)$, and $ (\bigcup_i T_i') \setminus (\bigcup_i R_i')$.
\item \label{itm:deletec}For each $i$ delete all points in $W_i$ covered by $R_i$, or covered by $R_i'$. 
\item \label{itm:deleted}For each $i$ delete all points in $K_i$ covered by $R$.
\item Finally delete $R$ from the list. Store the remaining points at $F'$.
\end{enumerate}
\item $R' \gets \emptyset$. Traverse the list as follows: For each $p_i$, check the distances from all $x_j$ s.t. 
$p_j\in [p_i-r,p_i]$.
\begin{itemize}
\item If $\exists\, x_j \in R' :$ $\|x_i-x_j\| \leq r$, delete $x_i$ from the list, set $F' \gets F' \backslash \{x_i , x_j\}$  and continue traversing the list. 
\item If there is no such point $x_j$ then $R \gets R \cup \{x_i\}$ and continue traversing the list.
\end{itemize}
\item Output indices of $R\gets R \cup R'$ and $F \gets F \cup F'$.
\end{enumerate}
\end{framed}

\begin{thm}\label{ThmSmallr}
For any constant $\epsilon>0$, $X\subset {\mathbb S}^{d-1}$ s.t. 
$|X|=n$, and $r < 1/n^{0.9}$, {\tt ApprxNet(Small radius)} will output
a $(1+\epsilon)r$-net and a set of $(1+\epsilon)$-approximate $r$-far points in time $\tilde{O}(dn^{2-\Theta(\sqrt[]{\epsilon})})$, with probability $1-o(1/n^{0.04})$.
\end{thm}

\begin{proof}
Note that points in $S_i$ had projections
$p_i$ in sets of contiguous intervals of width $r$; each interval had $\geq n^{0.6}$ points,
hence the diameter of the projection of $S_i$ is $\leq n^{0.4}r$.
By the Johnson Lindenstrauss Lemma \cite{DG02} we have that for $v \in {\mathbb S}^{d-1}$ chosen uniformly at random:
$${\mathrm Pr}\Big[\langle u,v\rangle^{2}\leq \frac{\|u\|^2}{n^{0.4}}\Big]\leq \frac{\sqrt{d} \sqrt{e}}{n^{0.2}}.
$$
Hence,
$
{\mathbb E}[| \{ x_k,x_j\in S_i \mid \|x_k-x_j\| \geq n^{0.6}r \text{ and }  \|p_k-p_j\|\leq n^{0.4} r\}|]
\leq |S_i|^2 \cdot  \frac{\sqrt{e d}}{n^{0.2}},
$
and the probability
$$
{\mathrm Pr}[| \{ x_k,x_j\in S_i \mid \|x_k-x_j\| \geq n^{0.6}r \text{ and }  \|p_k-p_j\|\leq n^{0.4} r\}| \geq |S_i|^{1.95}] \leq |S_i|^{0.05} \cdot  \frac{\sqrt{e d}}{n^{0.2}}\leq \frac{\sqrt{e d}}{n^{0.15}}.
$$
Taking a union bound over all sets $S_i$ yields a  probability of failure $o({1}/{n^{0.045}})$. 
This implies that (for large enough $n$, which implies large enough $|S_i|$) at least 
$${\binom{|S_i|}{2}} -|S_i|^{1.95}\geq {\frac{|S_i|^2}{4}}$$ 
distances between points in $S_i$ are indeed small 
($\leq n^{0.6}r$). Hence, there exists some point $p_k \in S_i$ which $(n^{0.6}r)$-covers $|S_i|/2$ points. For each possible $p_k$ 
we sample $n^{0.1}$ distances to other points, and by {Chernoff bounds}, if a point $(n^{0.6}r)$-covers a fraction of more than $1/2$ of the points in $S_i$, then it covers more than 
$n^{0.1}/3$ sampled points with high probability. Similarly, if 
a point $(n^{0.6}r)$-covers a fraction of less than $1/4$ of the points in $S_i$, then it covers less than 
$n^{0.1}/3$ sampled points with high probability.
More precisely, for some fixed $u\in S_i$, let $X_j=1$ 
when for the $j$th randomly chosen point $v\in S_i$, it holds 
$\| u-v\| \leq  n^{0.6}r$ and let $X_j=0$ otherwise. Then, for $Y=\sum_{j=1}^{n^{0.1}} X_j$, it holds:
$$
{\mathbb E}[Y]\geq n^{0.1}/2 \implies {\mathrm Pr}[Y\leq n^{0.1}/3 ]\leq \exp(- \Theta(n^{0.1})),
$$
$$
{\mathbb E}[Y]\leq n^{0.1}/4 \implies {\mathrm Pr}[Y\geq n^{0.1}/3]\leq \exp(- \Theta(n^{0.1})).
$$
Since for any point  $x\in T_i$ and any point $y \in T_i'$ we have $\|x-y\|>r$, the packing property of $r$-nets is preserved when we build $r$-nets for $T_i$ and $T_i'$ independently. For each $T_i$, we succeed in building $r$-nets with probability $1-O(1/n^{0.2})$. By a union bound over all sets $T_i$, we have a probability of failure $O(1/n^{0.1})$. 
Furthermore, points which belong to sets $W_i$ and $K_i$ are possibly covered and need to be checked.



For the analysis of the runtime of the algorithm, notice that step \ref{itm:deleteb} costs time 
$O(d\cdot (\sum_i|T_i|+\sum_i|T_i'|))=O(dn)$. Then, 
step \ref{itm:deletec} costs time $O(d \cdot \sum_i |W_i|\cdot |T_i|+d \cdot \sum_i |W_i|\cdot |T_i'|)=O(d n^{1.4})$. Finally, notice that we have at most $n^{0.1}$ sets $K_i$. Each $K_i$ contains at most $2n^{0.6}$ points, hence checking each point in $\bigcup_i K_i$ with each point in $R$ costs $O(d n^{1.7})$.


Now regarding step 5, consider any interval $[p_i-r,p_i]$ in the initial list, where all points are projected. If $|\{ j \mid p_j \in [p_i-r,p_i]\}\leq 2 n^{0.9}$ then the $i$th iteration in step 5 will obviously cost $O(n^{0.9})$, since previous steps only delete points. If $|\{ j \mid p_j \in [p_i-r,p_i]\}> 2 n^{0.9}$, we claim that 
$|\{j<i \mid  p_j \in [p_i-r,p_i] \text{ and } K_j \text{ is created}\}| \leq 1$. Consider the smallest $j <i$ s.t. $K_j$ is created and $p_j\in [p_i-r,p_i]$. This means that all points $p_k$, for $k\leq j$, are deleted when $p_j$ is visited. Now assume that there exists integer $l \in (j,i)$ s.t. $K_l$ is created. This means that the remaining points in the interval $[p_l-r,p_l]$ are  
$\leq n^{0.6}$ and all of the remaining points $p_k <p_l$ are more than $n^{0.9}$. This leads to contradiction, since by the deletion in the $j$th iteration, we know that all of the remaining points $p_k <p_l$ lie in the interval $[p_l-r,p_l]$. 

Now, assume that there exists one $ j<i$ s.t. $p_j \in [p_i-r,p_i]$ and $K_j$ is created. Then, when $p_i$ is visited, there at least $2 n^{0.9}-n^{0.6}>n^{0.9}$ remaining points in the interval $[p_i-r,p_i]$. Hence, there exists $l\geq i$  for which 
the remaining points in the interval $[p_i-r,p_i]$ 
are contained in $S_l \cup K_l$. Hence in this case, in step 5, there exist at most $O(n^{0.6})$ points which are not deleted and belong to the interval $[p_i-r,p_i]$. Now assume that there does not exist any $ j<i$ s.t. $p_j \in [p_i-r,p_i]$ and $K_j$ is created. This directly implies that  there exists $l\geq i$  for which 
the remaining points in the interval $[p_i-r,p_i]$ 
are contained in $S_l \cup K_l$.

At last, the total time of the above algorithm is dominated by the calls to the construction of the partial $r$-nets of the sets $T_i$. Thus, the total running time is $O(\sum_{ i} {|T_i|}^{2-\Theta(\sqrt{\epsilon})}+\sum_{ i} {|T_i|'}^{2-\Theta(\sqrt{\epsilon})})=
O(\sum_{ i} {|T_i|}^{2-\Theta(\sqrt{\epsilon}})+\sum_{i} {(3|T_i|/4)}^{2-\Theta(\sqrt{\epsilon})})=
\tilde{O}(n^{2-\Theta(\sqrt{\epsilon}))}).$
{Finally, taking a union bound over all recursive calls of the algorithm we obtain a probability of failure $o(1/n^{0.04})$.} 
\end{proof}

We now present an algorithm for an 
$(1+\epsilon)r$-net for points in ${\mathbb R}^d$ under Euclidean distance. 

\begin{framed}
{\tt ApprxNet}\\

Input: Matrix $X=[x_1,\ldots,x_n]$ with each $x_{i} \in {\mathbb R}^d$, parameter $r \in {\mathbb R}$, constant $\epsilon \in (0, 1/2]$.

Output: $R \subseteq \{x_1,\ldots,x_n\}$

\begin{itemize}
\item  Let $Y$, $r'$ be the output of algorithm {\tt Standardize} on input $X$, $r$ with parameter $\epsilon/4$.
\item If $r \geq 1/n^{0.9}$ run {\tt ApprxNet(Large radius)} on input $Y$,  $\epsilon/4, r'$ and return points which correspond to  the set $R$.
\item If $r < 1/n^{0.9}$ run {\tt ApprxNet(Small radius)} on input $Y$,  $\epsilon/4, r'$ and return points which correspond to the set $R$.
\end{itemize}
\end{framed}

\begin{thm}\label{ApprxNet}
Given $n$ points in ${\mathbb R}^d$, a distance parameter $r \in {\mathbb R}$ and an approximation parameter $\epsilon \in (0, 1/2]$, with probability $1-o(1/n^{0.04})$, {\tt ApprxNet} will return a $(1+\epsilon)r-net$, $R$, in $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$ time.
\end{thm}
\begin{proof}
The theorem is a direct implication of Theorems \ref{ThmLargeRadius}, \ref{ThmSmallr}, \ref{Stand}.
\end{proof}

\begin{thm}\label{DelFar}
Given $X\subset{\mathbb R}^d$ such that $|X|=n$, a distance parameter $r \in {\mathbb R}$ and an approximation parameter $\epsilon \in (0, 1/2]$, there exists an algorithm, {\tt DelFar}, that will return, with probability $1-o(1/n^{0.04})$, a set $F'$ with the following properties in $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$ time:
\begin{itemize}
\item If for a point $p \in X$ it holds that  $\forall q\neq p, q \in X$ we have $\|p-q\| > (1+\epsilon)r$, then $p \notin F'$.
\item If for a point $p \in X$ it holds that  $\exists q\neq p, q \in X$ s.t. $\|p-q\| \leq r$, then $p \in F'$.
\end{itemize}
\end{thm}



\section{Applications and Future work}\label{Sapps}

Concerning applications, in \cite{HR15}, they design an approximation scheme, which solves various
distance optimization problems. The technique employs a grid-based construction of $r$-nets which is linear in $n$, but exponential in $d$. The main prerequisite of the method is the existence of 
a linear-time decider (formally defined in Appendix~\ref{Aframework}). The framework is especially interesting when the dimension is constant, since the whole algorithm costs time linear in $n$ which, for some problems, improves upon previously known near-linear algorithms. 
When the dimension is high, we aim for polynomial dependency on $d$, and subquadratic dependency on $n$. 



Let us focus on the problem of approximating the {\it $k$th nearest neighbor distance}. 
\begin{dfn}
Let $X\subset {\mathbb R}^d$ be a set of $n$ points, approximation error $\epsilon>0$, and let $d_1\leq \ldots \leq d_n$ be the nearest neighbor distances. The problem of computing an $(1+\epsilon)$-approximation to the {\it $k$th nearest neighbor distance} asks for a pair 
$x,y\in X$ such that $\|x-y\|\in [(1-\epsilon)d_k,(1+\epsilon)d_k]$.
\end{dfn}

Now we present an approximate decider for the problem above. 
This procedure combined with the framework we mentioned earlier, which employs our net construction, results in an efficient solution for this problem in high dimension. 
\begin{framed}
{\tt kth NND Decider}\\

Input: $X \subseteq {\mathbb R}^d$, constant $\epsilon\in (0,1/2]$, integer $k>0$.

Output: An interval for the optimal value $f(X, k)$.
\begin{itemize}
\item Call {\tt DelFar}$(X, \frac{r}{1+\epsilon/4}, \epsilon/4)$ and store its output in $W_1$.
\item Call {\tt DelFar}$(X, r, \epsilon/4)$ and store its output in $W_2$.
\item Do one of the following:
\begin{itemize}
\item If $|W_1| > k$, then output $``f(X, k) < r"$.
\item If $|W_2| < k$, then output $``f(X, k) > r"$.
\item If $|W_1| \leq k$ and $\abs{W_2} \geq k$, then output $``f(X, k) \in [\frac{r}{1+\epsilon/4}, \frac{1+\epsilon/4}r]"$.
\end{itemize}
\end{itemize}
\end{framed}
 
\begin{thm}\label{KND}
Given a pointset $X \subseteq {\mathbb R}^d$, one can compute a $(1+\epsilon)$-approximation to the $k$-th nearest neighbor in $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$, with probability $1-o(1)$.
\end{thm}
To the best of our knowledge, this is the first high dimensional solution for this problem. 
Setting $k=n$ and applying Theorem \ref{KND} one can compute the {\it farthest nearest neighbor} in $\tilde{O}(dn^{2-\Theta(\sqrt{\epsilon})})$ with high probability.


Concerning future work, let us start with the problem of finding a greedy permutation. 
A permutation $\Pi = <\pi_1, \pi_2,\dots >$ of the vertices of a metric space $(X, \norm{\cdot})$ is a \textit{greedy permutation}  if each vertex $\pi_i$ is the farthest in $X$ from the set $\Pi_{i-1} = <{\pi_1,\dots, \pi_{i-1}}>$ of preceding vertices. The computation of $r$-nets is closely related to that of the greedy permutation.


The $k$-center clustering problem asks the following: given a set $X \subseteq {\mathbb R}^d$ and an integer $k$, find the smallest radius $r$ such that $X$ is contained within $k$ balls of radius $r$.
By \cite{EHS15}, a simple modification of our net construction implies an algorithm for the $(1+\epsilon)$ approximate greedy permutation in time $\tilde{O}(d n^{2-\Theta(\sqrt{\epsilon})} \log \Phi)$ where $\Phi$ denotes the spread of the pointset. 
Then, approximating the greedy permutation implies a 
$(2+\epsilon)$ approximation algorithm for $k$-center clustering problem. We expect that one can avoid any  dependencies on $\Phi$. 


\if 0
The Corollaries below follow from Theorem \ref{ApprxNet}, Lemma 3.5\cite{EHS15} and Lemma 2.1\cite{EHS15}.

\begin{cor} Let $X$ be a set of $n$ points in ${\mathbb R}^d$, $\epsilon \in (0, 1)$ an error parameter and let $\Phi$ be the spread of the
Euclidean metric on $X$. Then, one can compute in $O(dn^{2-\Theta(\sqrt{\epsilon})}\log \Phi)$
expected time a sequence
that is a $(1 + \epsilon)$-greedy permutation for the Euclidean metric on $X$, with high probability.
\end{cor} 

\begin{cor}
Given a set $X$ of $n$ points in ${\mathbb R}^d$, an integer $k$ and an error parameter $\epsilon \in (0, 1)$, one  can compute with high probability a $(2+\epsilon)$-approximation to the optimal $k$-center clustering in $O(dn^{2-\Theta(\sqrt{\epsilon})}\log \Phi)$, where $\Phi$ is the spread of the Euclidean metric on $X$.
\end{cor}\fi 

\subsection*{Acknowledgment.}
I.Z.~Emiris acknowledges partial support by the EU
H2020 research and innovation programme, under the Marie Sklodowska-Curie grant
agreement No 675789: Network ``ARCADES".



\newpage 
\bibliographystyle{alpha}

